version: "3.8"

services:
  backend:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: smartflow-backend
    ports:
      - "8000:8000"
    env_file:
      - ../.env
    volumes:
      - chroma_data:/app/data/chroma_db
      - ../data/sample_docs:/app/data/sample_docs
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8000/api/health')"]
      interval: 30s
      timeout: 10s
      retries: 3

  frontend:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: smartflow-frontend
    ports:
      - "8501:8501"
    environment:
      - BACKEND_URL=http://backend:8000
    command: streamlit run frontend/streamlit_app.py --server.port=8501 --server.address=0.0.0.0
    depends_on:
      backend:
        condition: service_healthy
    restart: unless-stopped

volumes:
  chroma_data:
version: "3.8"

services:
  backend:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: smartflow-backend
    ports:
      - "8000:8000"
    env_file:
      - ../.env
    volumes:
      - chroma_data:/app/data/chroma_db
      - ../data/sample_docs:/app/data/sample_docs
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8000/api/health')"]
      interval: 30s
      timeout: 10s
      retries: 3

  frontend:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: smartflow-frontend
    ports:
      - "8501:8501"
    env_file:
      - ../.env
    environment:
      - BACKEND_URL=http://backend:8000
    command: streamlit run frontend/streamlit_app.py --server.port=8501 --server.address=0.0.0.0
    depends_on:
      backend:
        condition: service_healthy
    restart: unless-stopped

  # Optional: Ollama for local LLM
  # Uncomment the following section if you want to use Ollama
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: smartflow-ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_models:/root/.ollama
  #   restart: unless-stopped
  #   # For GPU support, uncomment:
  #   # deploy:
  #   #   resources:
  #   #     reservations:
  #   #       devices:
  #   #         - driver: nvidia
  #   #           count: 1
  #   #           capabilities: [gpu]

volumes:
  chroma_data:
  # ollama_models:
